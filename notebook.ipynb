{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:09:38.919678Z",
     "iopub.status.busy": "2024-11-06T16:09:38.919274Z",
     "iopub.status.idle": "2024-11-06T16:09:52.644722Z",
     "shell.execute_reply": "2024-11-06T16:09:52.643718Z",
     "shell.execute_reply.started": "2024-11-06T16:09:38.919640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:15:57.749063Z",
     "iopub.status.busy": "2024-11-06T16:15:57.748274Z",
     "iopub.status.idle": "2024-11-06T16:15:57.753497Z",
     "shell.execute_reply": "2024-11-06T16:15:57.752473Z",
     "shell.execute_reply.started": "2024-11-06T16:15:57.749024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import gc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:10:04.882970Z",
     "iopub.status.busy": "2024-11-06T16:10:04.882405Z",
     "iopub.status.idle": "2024-11-06T16:10:05.010687Z",
     "shell.execute_reply": "2024-11-06T16:10:05.009720Z",
     "shell.execute_reply.started": "2024-11-06T16:10:04.882934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:10:08.515254Z",
     "iopub.status.busy": "2024-11-06T16:10:08.514821Z",
     "iopub.status.idle": "2024-11-06T16:10:12.130581Z",
     "shell.execute_reply": "2024-11-06T16:10:12.129626Z",
     "shell.execute_reply.started": "2024-11-06T16:10:08.515212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc8ae420d7843b9bda04bf43deabaf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b8f34bb881421291e9e3bcacfbb8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35318b08772f4ded808d01304801d982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/16.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bc622f763c488a80d9c3d34f3023c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/5.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8900e2c2218046dc8a0f55e080915f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev-00000-of-00001.parquet:   0%|          | 0.00/5.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37db6ee33ab4dbfabf3200fa757df98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fad7b67b5640bba04fd9aef255c4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d4b04091ab4920aa7e38fb6551f5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"cais/mmlu\", \"college_mathematics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:45:34.071294Z",
     "iopub.status.busy": "2024-11-06T16:45:34.070883Z",
     "iopub.status.idle": "2024-11-06T16:45:34.077112Z",
     "shell.execute_reply": "2024-11-06T16:45:34.076136Z",
     "shell.execute_reply.started": "2024-11-06T16:45:34.071240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Map index to letter (0 -> A, 1 -> B, 2 -> C, 3 -> D)\n",
    "def index_to_letter(index):\n",
    "    return [\"A\", \"B\", \"C\", \"D\"][index]\n",
    "\n",
    "# Helper function to parse a single-letter answer (A, B, C, or D)\n",
    "def parse_answer(output):\n",
    "    match = re.search(r'\\b[A-D]\\b', output)\n",
    "    return match.group(0) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:10:15.867707Z",
     "iopub.status.busy": "2024-11-06T16:10:15.866728Z",
     "iopub.status.idle": "2024-11-06T16:10:15.874339Z",
     "shell.execute_reply": "2024-11-06T16:10:15.873260Z",
     "shell.execute_reply.started": "2024-11-06T16:10:15.867663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    \"google/gemma-2b-it\",\n",
    "    \"microsoft/Phi-3.5-mini-instruct\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:46:09.181345Z",
     "iopub.status.busy": "2024-11-06T16:46:09.180472Z",
     "iopub.status.idle": "2024-11-06T16:46:09.187461Z",
     "shell.execute_reply": "2024-11-06T16:46:09.186500Z",
     "shell.execute_reply.started": "2024-11-06T16:46:09.181302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def zero_shot_inference(model, tokenizer, question, choices):\n",
    "    options_text = \"\\n\".join([f\"{index_to_letter(i)}: {choice}\" for i, choice in enumerate(choices)])\n",
    "    prompt = f\"Choose the correct answer from the options below. Only respond with a single letter: A, B, C, or D.\\nQuestion: {question}\\nOptions:\\n{options_text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:46:23.554621Z",
     "iopub.status.busy": "2024-11-06T16:46:23.553862Z",
     "iopub.status.idle": "2024-11-06T16:46:23.561022Z",
     "shell.execute_reply": "2024-11-06T16:46:23.559769Z",
     "shell.execute_reply.started": "2024-11-06T16:46:23.554580Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Chain of Thought\n",
    "def chain_of_thought_inference(model, tokenizer, question, choices):\n",
    "    options_text = \"\\n\".join([f\"{index_to_letter(i)}: {choice}\" for i, choice in enumerate(choices)])\n",
    "    prompt = f\"Choose the correct answer for the question below by thinking step by step. Respond with a single letter: A, B, C, or D.\\nQuestion: {question}\\nOptions:\\n{options_text}\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:46:33.424419Z",
     "iopub.status.busy": "2024-11-06T16:46:33.423498Z",
     "iopub.status.idle": "2024-11-06T16:46:33.431147Z",
     "shell.execute_reply": "2024-11-06T16:46:33.430118Z",
     "shell.execute_reply.started": "2024-11-06T16:46:33.424364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 3. ReAct Prompting\n",
    "def react_prompt_inference(model, tokenizer, question, choices):\n",
    "    options_text = \"\\n\".join([f\"{index_to_letter(i)}: {choice}\" for i, choice in enumerate(choices)])\n",
    "    prompt = f\"Question: {question}\\nOptions:\\n{options_text}\\nAnswer with reasoning steps, then select a single letter answer (A, B, C, or D).\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:45:49.750037Z",
     "iopub.status.busy": "2024-11-06T16:45:49.749662Z",
     "iopub.status.idle": "2024-11-06T16:45:49.758536Z",
     "shell.execute_reply": "2024-11-06T16:45:49.757348Z",
     "shell.execute_reply.started": "2024-11-06T16:45:49.750001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, questions, answers, prompt_func):\n",
    "    correct = 0\n",
    "    total_time = 0.0\n",
    "\n",
    "    for question, answer_index in zip(questions, answers):\n",
    "        start_time = time.time()\n",
    "        generated_answer = prompt_func(model, tokenizer, question[\"question\"], question[\"choices\"])\n",
    "        total_time += time.time() - start_time\n",
    "\n",
    "        # Convert answer index to letter for comparison\n",
    "        correct_answer = index_to_letter(answer_index)\n",
    "        parsed_answer = parse_answer(generated_answer)\n",
    "\n",
    "        if parsed_answer == correct_answer:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(questions)\n",
    "    avg_time = total_time / len(questions)\n",
    "    return accuracy, avg_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:14:05.367268Z",
     "iopub.status.busy": "2024-11-06T16:14:05.366184Z",
     "iopub.status.idle": "2024-11-06T16:14:05.372523Z",
     "shell.execute_reply": "2024-11-06T16:14:05.371486Z",
     "shell.execute_reply.started": "2024-11-06T16:14:05.367225Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Let k be the number of real solutions of the equation e^x + x - 2 = 0 in the interval [0, 1], and let n be the number of real solutions that are not in [0, 1]. Which of the following is true?', 'subject': 'college_mathematics', 'choices': ['k = 0 and n = 1', 'k = 1 and n = 0', 'k = n = 1', 'k > 1'], 'answer': 1}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:46:51.322893Z",
     "iopub.status.busy": "2024-11-06T16:46:51.322516Z",
     "iopub.status.idle": "2024-11-06T16:46:51.337197Z",
     "shell.execute_reply": "2024-11-06T16:46:51.336180Z",
     "shell.execute_reply.started": "2024-11-06T16:46:51.322859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Sample a subset of questions for evaluation (to save time and resources)\n",
    "sample_data = dataset[\"test\"].select(range(10))\n",
    "sample_questions = [{\"question\": item[\"question\"], \"choices\": item[\"choices\"]} for item in sample_data]\n",
    "sample_answers = [item[\"answer\"] for item in sample_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:25:51.607916Z",
     "iopub.status.busy": "2024-11-06T16:25:51.607527Z",
     "iopub.status.idle": "2024-11-06T16:25:51.613127Z",
     "shell.execute_reply": "2024-11-06T16:25:51.611864Z",
     "shell.execute_reply.started": "2024-11-06T16:25:51.607881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T16:47:21.291445Z",
     "iopub.status.busy": "2024-11-06T16:47:21.291005Z",
     "iopub.status.idle": "2024-11-06T16:53:37.091253Z",
     "shell.execute_reply": "2024-11-06T16:53:37.090304Z",
     "shell.execute_reply.started": "2024-11-06T16:47:21.291404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model google/gemma-2b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7894176389d4af2a5ed60b2b42bb160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for google/gemma-2b-it\n",
      "\n",
      "Loading model microsoft/Phi-3.5-mini-instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f90c1cea5c1442ba513e56bb0d05894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for microsoft/Phi-3.5-mini-instruct\n",
      "\n",
      "Loading model meta-llama/Meta-Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf6285be65b48cbb9f0c6de64f81ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed evaluation for meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Results for model google/gemma-2b-it:\n",
      "  Zero Shot - Accuracy: 0.20, Avg Inference Time: 0.90 seconds\n",
      "  Chain of Thought - Accuracy: 0.20, Avg Inference Time: 2.03 seconds\n",
      "  ReAct - Accuracy: 0.30, Avg Inference Time: 2.03 seconds\n",
      "\n",
      "\n",
      "Results for model microsoft/Phi-3.5-mini-instruct:\n",
      "  Zero Shot - Accuracy: 0.20, Avg Inference Time: 3.62 seconds\n",
      "  Chain of Thought - Accuracy: 0.20, Avg Inference Time: 3.62 seconds\n",
      "  ReAct - Accuracy: 0.30, Avg Inference Time: 3.61 seconds\n",
      "\n",
      "\n",
      "Results for model meta-llama/Meta-Llama-3.1-8B-Instruct:\n",
      "  Zero Shot - Accuracy: 0.20, Avg Inference Time: 3.50 seconds\n",
      "  Chain of Thought - Accuracy: 0.20, Avg Inference Time: 3.48 seconds\n",
      "  ReAct - Accuracy: 0.30, Avg Inference Time: 3.48 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluations model-by-model to avoid memory issues\n",
    "for model_name in model_paths:\n",
    "    print(f\"Loading model {model_name}...\")\n",
    "    \n",
    "    # Load model and tokenizer with authentication if needed (replace 'your_token' if using a gated model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Store results for the current model\n",
    "    model_results = {}\n",
    "\n",
    "    # Zero Shot\n",
    "    accuracy, avg_time = evaluate_model(model, tokenizer, sample_questions, sample_answers, zero_shot_inference)\n",
    "    model_results[\"Zero Shot\"] = {\"accuracy\": accuracy, \"avg_time\": avg_time}\n",
    "    \n",
    "    # Chain of Thought\n",
    "    accuracy, avg_time = evaluate_model(model, tokenizer, sample_questions, sample_answers, chain_of_thought_inference)\n",
    "    model_results[\"Chain of Thought\"] = {\"accuracy\": accuracy, \"avg_time\": avg_time}\n",
    "    \n",
    "    # ReAct Prompting\n",
    "    accuracy, avg_time = evaluate_model(model, tokenizer, sample_questions, sample_answers, react_prompt_inference)\n",
    "    model_results[\"ReAct\"] = {\"accuracy\": accuracy, \"avg_time\": avg_time}\n",
    "    \n",
    "    # Save results for this model\n",
    "    results[model_name] = model_results\n",
    "\n",
    "    # Unload the model and clear memory\n",
    "    del model\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Completed evaluation for {model_name}\\n\")\n",
    "\n",
    "# Display results for each model and prompting method\n",
    "for model_name, result in results.items():\n",
    "    print(f\"Results for model {model_name}:\")\n",
    "    for prompt_style, metrics in result.items():\n",
    "        print(f\"  {prompt_style} - Accuracy: {metrics['accuracy']:.2f}, Avg Inference Time: {metrics['avg_time']:.2f} seconds\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
